{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c580b53-b9b0-4703-b45a-e83faaf5e071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/bin/python\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, random_split\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Set CUDA device to GPU #7\n",
    "torch.cuda.set_device(7)\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class InterferogramDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Grayscale(1)\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        name = name.replace('n', '-').replace('p', '.')\n",
    "        parts = name.split('_')\n",
    "        params = np.zeros(8)\n",
    "        for i, part in enumerate(parts[1:9]):\n",
    "            params[i] = float(part[1:])\n",
    "            \n",
    "        return image, torch.FloatTensor(params)\n",
    "\n",
    "def get_data_loaders(root_folder, batch_size=32, num_workers=8):\n",
    "    \"\"\"Create data loaders that efficiently load images in small chunks.\"\"\"\n",
    "    file_paths = glob.glob(os.path.join(root_folder, '*.jpg'))\n",
    "    dataset = InterferogramDataset(file_paths)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "class InterferogramNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InterferogramNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "            \"\"\"Creates a Conv2D -> BatchNorm -> ReLU block\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(1, 32),\n",
    "            conv_block(32, 32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            conv_block(32, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            conv_block(64, 128),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            conv_block(256, 256),\n",
    "            conv_block(256, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            conv_block(256, 512),\n",
    "            conv_block(512, 512),\n",
    "            conv_block(512, 512),\n",
    "            conv_block(512, 512),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def preprocess_image(filename):\n",
    "    image = Image.open(filename).convert('L')\n",
    "    image = np.array(image, dtype=np.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def output_function(epoch, iteration, train_loss, train_rmse, val_loss=None, val_rmse=None, \n",
    "                   time_since_start=0, learning_rate=0):\n",
    "    if iteration % 100 == 0:\n",
    "        print(f'\\n=== Training Progress at Iteration {iteration} ===')\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Training Loss: {train_loss:.6f}')\n",
    "        print(f'Training RMSE: {train_rmse:.6f}')\n",
    "        if val_loss is not None:\n",
    "            print(f'Validation Loss: {val_loss:.6f}')\n",
    "            print(f'Validation RMSE: {val_rmse:.6f}')\n",
    "        print(f'Time Since Start: {time_since_start/60:.2f} minutes')\n",
    "        print(f'Current Learning Rate: {learning_rate:.6f}')\n",
    "        print('===================================\\n')\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\" Using device: {device}\")\n",
    "\n",
    "    train_loader, val_loader = get_data_loaders('training3', batch_size=400, num_workers=4)\n",
    "\n",
    "    \n",
    "    model = InterferogramNet().to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    iteration = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            rmse = torch.sqrt(loss)\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                val_rmse = 0\n",
    "                with torch.no_grad():\n",
    "                    for val_images, val_targets in val_loader:\n",
    "                        val_images, val_targets = val_images.to(device), val_targets.to(device)\n",
    "                        val_outputs = model(val_images)\n",
    "                        val_batch_loss = criterion(val_outputs, val_targets)\n",
    "                        val_loss += val_batch_loss.item()\n",
    "                        val_rmse += torch.sqrt(val_batch_loss).item()\n",
    "                \n",
    "                val_loss /= len(val_loader)\n",
    "                val_rmse /= len(val_loader)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= 1000:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "                \n",
    "                output_function(epoch, iteration, loss.item(), rmse.item(),\n",
    "                              val_loss, val_rmse, iteration * 0.1, optimizer.param_groups[0]['lr'])\n",
    "                \n",
    "                model.train()\n",
    "    \n",
    "    save_dir = 'models'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(save_dir, 'trained_network.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved as: {model_path}')\n",
    "    \n",
    "    print('\\n=== Testing Some Predictions ===')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_images, actual_params = next(iter(val_loader))\n",
    "        test_images, actual_params = test_images.to(device), actual_params.to(device)\n",
    "        predictions = model(test_images)\n",
    "        \n",
    "        for i in range(min(5, predictions.size(0))):\n",
    "            print(f'\\nSample {i+1}:')\n",
    "            print(f'Predicted: D={predictions[i,0]:.4f}, C={predictions[i,1]:.4f}, '\n",
    "                  f'B={predictions[i,2]:.4f}, G={predictions[i,3]:.4f}, '\n",
    "                  f'F={predictions[i,4]:.4f}, J={predictions[i,5]:.4f}, '\n",
    "                  f'E={predictions[i,6]:.4f}, I={predictions[i,7]:.4f}')\n",
    "            print(f'Actual:    D={actual_params[i,0]:.4f}, C={actual_params[i,1]:.4f}, '\n",
    "                  f'B={actual_params[i,2]:.4f}, G={actual_params[i,3]:.4f}, '\n",
    "                  f'F={actual_params[i,4]:.4f}, J={actual_params[i,5]:.4f}, '\n",
    "                  f'E={actual_params[i,6]:.4f}, I={actual_params[i,7]:.4f}')\n",
    "            \n",
    "            errors = torch.abs(predictions[i] - actual_params[i])\n",
    "            print(f'Abs Error: D={errors[0]:.4f}, C={errors[1]:.4f}, '\n",
    "                  f'B={errors[2]:.4f}, G={errors[3]:.4f}, '\n",
    "                  f'F={errors[4]:.4f}, J={errors[5]:.4f}, '\n",
    "                  f'E={errors[6]:.4f}, I={errors[7]:.4f}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665851e4-61bd-4b89-b5ef-e0e7e7a011f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenvAN)",
   "language": "python",
   "name": "myenvan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
